{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6bac4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LIBRERIAS ---\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a464181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos con datos piezometricos encontrados: 6\n",
      "['data/raw/piezometers\\\\Data_08_11_2024_SDH2PS01_COMPENSADA.xlsx', 'data/raw/piezometers\\\\Data_15_07_2025_ SDH1PS01_COMPENSADA.xlsx', 'data/raw/piezometers\\\\Data_15_07_2025_ SDH1PS02_COMPENSADA.xlsx', 'data/raw/piezometers\\\\Data_15_07_2025_SDH2PP01_COMPENSADA.xlsx', 'data/raw/piezometers\\\\Data_15_07_2025_SDH2PS02_COMPENSADA.xlsx', 'data/raw/piezometers\\\\Data_15_07_2025_SDH2PS03_COMPENSADA.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURACIONES MODIFICABLES ---\n",
    "\n",
    "# 1. RUTA A LOS DATOS\n",
    "# Busca todos los archivos que terminen en '*_COMPENSADA.xlsx' en la carpeta \n",
    "# especificada y los almacena en una lista\n",
    "data_path = 'data/raw/piezometers/'\n",
    "piezometer_files = glob.glob(os.path.join(data_path, '*_COMPENSADA.xlsx'))\n",
    "\n",
    "# Imprime los archivos encontrados\n",
    "print(f'Archivos con datos piezometricos encontrados: {len(piezometer_files)}')\n",
    "print(piezometer_files)\n",
    "\n",
    "# 2. NOMBRES DE COLUMNAS\n",
    "# Define un diccionario con nombres de columnas originales y a renombrar\n",
    "columns_dict = {\n",
    "    'TEMPERATURE' : 'Temperature_C',\n",
    "    'NE_m' : 'Depth_m',\n",
    "    'Cota_m' : 'Static_level_masl'\n",
    "}\n",
    "\n",
    "# Define una lista con con nombres de columnas no necesarias\n",
    "columns_list = [\n",
    "    'ms',\n",
    "    'LEVEL',\n",
    "    'P_baro'\n",
    "]\n",
    "\n",
    "# 3. CAMPANAS DE TERRENO\n",
    "# Define un diccionario con el nombre y rango de fechas de cada campana.\n",
    "# La fecha de termino es excluyente. Se consideran fechas de trabajo efectivo.\n",
    "field_campaigns = {\n",
    "    \"May 2024\": pd.date_range(start='2024-05-21', end='2024-05-23'),\n",
    "    \"Jul 2024\": pd.date_range(start='2024-07-25', end='2024-07-28'),\n",
    "    \"Sep 2024\": pd.date_range(start='2024-09-03', end='2024-09-07'),\n",
    "    \"Nov 2024\": pd.date_range(start='2024-11-05', end='2024-11-12'),\n",
    "    \"Jan 2025\": pd.date_range(start='2025-01-21', end='2025-01-23'),\n",
    "    \"Apr 2025\": pd.date_range(start='2025-04-28', end='2025-05-02'),\n",
    "    \"Jul 2025\": pd.date_range(start='2025-07-08', end='2025-07-16')\n",
    "}\n",
    "\n",
    "# 4. MAYOR INTERVALO DE MEDICION DE LOS DATALOGGERS\n",
    "max_expected_interval='15min'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc0014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEFINICION DE FUNCIONES ---\n",
    "\n",
    "# 1. MANEJO DE FECHAS E INDICE\n",
    "\n",
    "def timestamp_as_index(df):\n",
    "    \"\"\"Convierte las columnas 'Date' y 'Time' en un indice Datetime\"\"\"\n",
    "\n",
    "    # Copia el df original\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Convierte las columnas 'Date' y 'Time' en strings\n",
    "    date_str = df_copy['Date'].astype(str)\n",
    "    time_str = df_copy['Time'].astype(str)\n",
    "\n",
    "    # Crea la columna 'Timestamps' con formato datetime a partir de 'Date' y 'Time'\n",
    "    df_copy['Timestamps'] = pd.to_datetime(\n",
    "        date_str + ' ' + time_str,\n",
    "        format='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "    # Establece 'Timestamps' como indice\n",
    "    df_copy = df_copy.set_index('Timestamps')\n",
    "\n",
    "    # Elimina las columnas 'Date' y 'Time'\n",
    "    df_copy = df_copy.drop(columns=['Date', 'Time'])\n",
    "\n",
    "    print('Indice datetime establecido')\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "\n",
    "# 2. FORMATEO DE COLUMNAS\n",
    "\n",
    "def format_columns(df):\n",
    "    \"\"\"Renombra columnas y elimina las innecesarias\"\"\"\n",
    "    df_formatted = df.rename(columns=columns_dict).drop(columns=columns_list, axis=1, errors='ignore')\n",
    "    print('Nombres de columnas formateados')\n",
    "    return df_formatted\n",
    "\n",
    "\n",
    "\n",
    "# 3. IDENTIFICACION DE DATOS DUPLICADOS\n",
    "\n",
    "def check_duplicates(df):\n",
    "    \"\"\"Revisa e informa sobre datos duplicados en el indice\"\"\"\n",
    "\n",
    "    # Genera una serie booleana que almacena los indices duplicados como True\n",
    "    duplicates = df.index.duplicated(keep=False)\n",
    "\n",
    "    # Comprueba si la serie duplicates tiene algun valor True\n",
    "    if not duplicates.any():\n",
    "        print('No hay datos duplicados')\n",
    "    # De haberlos, imprime el total y a cuales registros corresponde\n",
    "    else:\n",
    "        print(f'Hay {duplicates.sum()} datos duplicados:')\n",
    "        print(df[duplicates])\n",
    "\n",
    "\n",
    "\n",
    "# 4. IDENTIFICACION DE SALTOS REGULARES Y ANOMALOS EN LOS DATOS\n",
    "\n",
    "def check_discontinuities(df, max_expected_interval):\n",
    "    \"\"\"Revisa e informa sobre saltos de tiempo en el indice\"\"\"\n",
    "    \n",
    "    # Genera una serie Timedelta que almcacena el tiempo transcurrido desde el registro anterior\n",
    "    intervals = df.index.to_series().diff()\n",
    "\n",
    "    # Genera una nueva serie con los Timedelta que superan un intervalo maximo esperado\n",
    "    interval_anomalies = intervals[intervals > pd.Timedelta(max_expected_interval)]\n",
    "\n",
    "    # Imprime los intervalos mas comunes y su frecuencia\n",
    "    print(f'\\nConteo de intervalos:\\n{intervals.value_counts().head()}')\n",
    "\n",
    "    # Comprueba si la serie interval_anomalies tiene algun valor\n",
    "    if interval_anomalies.empty:\n",
    "        print(f'\\nNo hay intervalos anomalos')\n",
    "    # De ser asi, imprime los registros con intervalos anomalos\n",
    "    else:\n",
    "        print(f'\\nIntervalos anÃ³malos:\\n{interval_anomalies}')\n",
    "\n",
    "\n",
    "\n",
    "# 5. IDENTIFICACION DE DATOS ANOMALOS DURANTE CAMPANAS DE TERRENO\n",
    "\n",
    "def check_outliers(df, campaign_dates, well_name, campaign_name):\n",
    "    \"\"\"Identifica, informa y visualiza outliers en un rango de fechas\"\"\"\n",
    "\n",
    "    # Hace una copia del df y lo filtra a las fechas de la campana de terreno\n",
    "    df_copy = df.copy()\n",
    "    df_campaign = df_copy.loc[campaign_dates.min() : campaign_dates.max()- pd.Timedelta(seconds=1)].copy()\n",
    "\n",
    "    # Si no hay datos durante la campana se detiene la funcion\n",
    "    if df_campaign.empty:\n",
    "        print(\"No se encontraron datos para esta campana\")\n",
    "        return\n",
    "\n",
    "    # Calcula el z-score de los valores de Temperature_C y Depth_m durante la campana\n",
    "    df_campaign['z_temp'] = stats.zscore(df_campaign['Temperature_C'])\n",
    "    df_campaign['z_depth'] = stats.zscore(df_campaign['Depth_m'])\n",
    "\n",
    "    # Crea un df con registros que tengan z-scores > 3\n",
    "    outlier_condition = (abs(df_campaign['z_temp']) > 3) | (abs(df_campaign['z_depth']) > 3)\n",
    "    df_outliers = df_campaign[outlier_condition]\n",
    "\n",
    "    # Comprueba si el df_outliers tiene algun valor anomalo\n",
    "    if not df_outliers.empty:\n",
    "        print(\"\\nOutliers detectados:\")\n",
    "        print(df_outliers[['Temperature_C', 'Depth_m', 'z_temp', 'z_depth']])\n",
    "    else:\n",
    "        print(\"\\nNo se encontraron valores con z-Score > 3 en esta campana.\")\n",
    "\n",
    "    # Grafica los valores normalizados de Temperature_C y Depth_m\n",
    "    title = f\"{well_name} - {campaign_name}\"\n",
    "    ax = df_campaign[['z_temp', 'z_depth']].plot(\n",
    "        figsize=(10, 4),\n",
    "        title=title,\n",
    "        grid=True\n",
    "    )\n",
    "    ax.axhline(3, color='r', linestyle='--', lw=0.8)\n",
    "    ax.axhline(-3, color='r', linestyle='--', lw=0.8)\n",
    "    ax.set_ylabel('Z-Score')\n",
    "    ax.set_xlabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d22aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BUCLE DE PROCESAMIENTO ---\n",
    "\n",
    "# Define una carpeta de salida para los datos limpios y la crea si no existe\n",
    "output_cleaned_path = 'data/processed/piezometers/cleaned/'\n",
    "os.makedirs(output_cleaned_path, exist_ok=True)\n",
    "\n",
    "# Bucle externo: itera sobre cada pozo\n",
    "# file_path refiere a cada valor de la lista piezometer_files\n",
    "for file_path in piezometer_files:\n",
    "    \n",
    "    # Define el nombre de cada pozo usando el nombre de archivo.\n",
    "    # Mantiene la penultima cadena de texto, separadas por guion bajo\n",
    "    base_name = os.path.basename(file_path)\n",
    "    well_name = base_name.split('_')[-2].strip()\n",
    "    \n",
    "    # Imprime un titulo que indica el nombre del pozo, de archivo y rango de fechas\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"POZO: {well_name}\")\n",
    "    print(f\"Archivo: {os.path.basename(file_path)}\")\n",
    "    print(\"=\"*80)\n",
    " \n",
    "\n",
    "    # Aplica las funciones de formateo de fechas y columnas\n",
    "    print(\"\\n--- Lectura y formateo de datos ---\")\n",
    "    try:\n",
    "        df_raw = pd.read_excel(file_path)\n",
    "        df_processed = timestamp_as_index(df_raw)\n",
    "        df_processed = format_columns(df_processed)\n",
    "        print(f\"\\nRango de fechas: {df_processed.index.min()} - {df_processed.index.max()}\")\n",
    "\n",
    "    # En caso de error, avisa cual archivo no se pudo procesar y continua el bucle  \n",
    "    except:\n",
    "        print(f\"ERROR: No se pudo procesar el archivo {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Genera un nombre de archivo y asigna ubicacion al dataframe formateado\n",
    "    output_filename = os.path.join(output_cleaned_path, f'{well_name}_cleaned.csv')\n",
    "    df_processed.to_csv(output_filename)\n",
    "    print(f\"\\nDatos limpios para {well_name} guardados en: {output_filename}\")\n",
    "\n",
    "\n",
    "    # Aplica las funciones de diagnostico de datos aplicables a todo el archivo\n",
    "    print(\"\\n--- Diagnostico de datos ---\")\n",
    "    check_duplicates(df_processed)\n",
    "    check_discontinuities(df_processed, max_expected_interval)\n",
    "\n",
    "    # Bucle interno: itera sobre las campanas de terreno. \n",
    "    # name y dates refieren a los pares de valores almacenados en el diccionario field_campaigns\n",
    "    for campaing_name, campaign_dates in field_campaigns.items():\n",
    "\n",
    "        # Imprime el nombre de la campana\n",
    "        print(f\"\\n--- Diagnostico campana {campaing_name}---\")\n",
    "\n",
    "        # Aplica la funcion de identificacion de outliers\n",
    "        print(\"Identificacion de outliers\")\n",
    "        check_outliers(df_processed, campaign_dates, well_name, campaing_name)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
